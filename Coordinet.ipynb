{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Coordinet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO17GFML0Z1+cEcP+VctGgf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jezgillen/coordinet/blob/main/Coordinet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "c19o0pwkA8G_",
        "outputId": "ec610e8c-4003-4b6f-c740-e1ed9d003eee"
      },
      "source": [
        "#!/usr/bin/python3\n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pickle\n",
        "#import interrupt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class FullyConnected(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_sizes):\n",
        "        ''' arguments: int, int, list<int> '''\n",
        "        super().__init__()\n",
        "        # declare parameters\n",
        "        self.w = nn.ModuleList() # list of weight matricies\n",
        "\n",
        "        # this loop initialises all weights except last layer\n",
        "        for h in hidden_sizes:\n",
        "            self.w.append(nn.Linear(input_size, h))\n",
        "            input_size = h\n",
        "\n",
        "        self.w.append(nn.Linear(input_size, output_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch = x.shape[0]\n",
        "        x = torch.reshape(x,(batch, -1))\n",
        "        # define computation\n",
        "        for w in self.w[:-1]:\n",
        "            x = F.relu(w(x))\n",
        "\n",
        "        return self.w[-1](x) # this outputs the logits\n",
        "\n",
        "    def predict(self, x):\n",
        "        return F.softmax(self.forward(x))\n",
        "\n",
        "    def num_parameters(self):\n",
        "        num_parameters = 0\n",
        "        for p in self.parameters():\n",
        "            num_parameters += np.prod(p.shape)\n",
        "        return num_parameters\n",
        "\n",
        "class Coordinet(nn.Module):\n",
        "    def __init__(self, input_size, embedding_size, output_size, embed_hidden_sizes,decode_hidden_sizes):\n",
        "        ''' arguments: int, int, list<int> '''\n",
        "        super().__init__()\n",
        "        # declare parameters\n",
        "        self.w = nn.ModuleList() # list of weight matricies\n",
        "\n",
        "        # this loop initialises all weights except last layer\n",
        "        for h in embed_hidden_sizes:\n",
        "            self.w.append(nn.Linear(input_size, h))\n",
        "            input_size = h\n",
        "\n",
        "        self.w.append(nn.Linear(input_size, embedding_size))\n",
        "\n",
        "        self.embed_to_output = FullyConnected(embedding_size, output_size, decode_hidden_sizes)\n",
        "        self.w.append(self.embed_to_output)\n",
        "\n",
        "    def coordinate_encoder(self, x):\n",
        "        ''' takes image x of shape [batch, d, h, w] and maps to [batch, d+2, h*w] '''\n",
        "        shape = x.shape\n",
        "        # create coordinates of shape [batch, h, w, 2]\n",
        "        h = torch.arange(0,shape[2])\n",
        "        w = torch.arange(0,shape[3])\n",
        "        h = torch.transpose(torch.tile(h,(shape[0],1,shape[3],1)),3,2)\n",
        "        w = torch.tile(w,(shape[0],1,shape[2],1))\n",
        "        coords = torch.cat([h,w],dim=1).cuda()\n",
        "        coords = coords/28\n",
        "        x = torch.cat([x.cuda(),coords],dim=1)\n",
        "        x = torch.reshape(x, (shape[0], x.shape[1], shape[2]*shape[3]))\n",
        "        x = torch.transpose(x,1,2)\n",
        "        return x\n",
        "\n",
        "    def process_pixels(self, x):\n",
        "        # define computation\n",
        "        for w in self.w[:-2]:\n",
        "            x = F.relu(w(x))\n",
        "\n",
        "        return self.w[-2](x) # this outputs the logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.coordinate_encoder(x)\n",
        "\n",
        "        # (pixel -> embedding) module\n",
        "        # [batch, h*w, d] -> [batch, h*w, embedding]\n",
        "        batch, hxw, depth = x.shape\n",
        "        x = torch.reshape(x, (-1, depth))\n",
        "        embeddings = self.process_pixels(x)\n",
        "        embeddings = torch.reshape(embeddings,(batch,hxw,-1))\n",
        "        embedding = torch.sum(embeddings, dim=1)\n",
        "\n",
        "        # (embedding -> output) module\n",
        "        # [batch, embedding] -> [batch, output]\n",
        "        output = self.embed_to_output(embedding)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def predict(self, x):\n",
        "        return F.softmax(self.forward(x))\n",
        "\n",
        "    def num_parameters(self):\n",
        "        num_parameters = 0\n",
        "        for p in self.parameters():\n",
        "            num_parameters += np.prod(p.shape)\n",
        "        return num_parameters\n",
        "\n",
        "\n",
        "def training_loop(model, train_dataset, test_dataset):\n",
        "\n",
        "    # Data loader\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                               batch_size=batch_size, \n",
        "                                               shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                              batch_size=batch_size, \n",
        "                                              shuffle=False)\n",
        "\n",
        "    # set up for training loop\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,)# momentum=momentum)\n",
        "    loss_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    #  print(\"Num Parameters: \", model.num_parameters())\n",
        "\n",
        "    curr_loss = 10e8\n",
        "\n",
        "    # training loop\n",
        "    epoch = 0\n",
        "    while(curr_loss > LOSS_THRESHOLD):\n",
        "        curr_loss = 0\n",
        "        num_training_images = 0\n",
        "        for i, (X, y) in enumerate(train_loader):\n",
        "            X = X.cuda()\n",
        "            y = y.cuda()\n",
        "            #  Forward pass\n",
        "            outputs = model(X)\n",
        "            loss = loss_criterion(outputs, y)\n",
        "            curr_loss += loss.item()*y.shape[0] #curr batch size\n",
        "            num_training_images += y.shape[0]\n",
        "            \n",
        "            # Backprpagation and optimization\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        curr_loss /= num_training_images\n",
        "        print(f\"\\r Epoch {epoch+1}, Average Loss: {curr_loss}\",end='')\n",
        "        epoch += 1\n",
        "        if(BREAK_TRAINING): \n",
        "            break\n",
        "        if(epoch > 10000):\n",
        "            print(\"*********************** ERROR ******************************\")\n",
        "            print(\" ************************ got stuck ***********************\")\n",
        "            break\n",
        "    print()\n",
        "\n",
        "    # testing\n",
        "    with torch.no_grad():\n",
        "        total_correct = 0\n",
        "        total_loss = 0\n",
        "        total_images = 0\n",
        "        for X, y in train_loader:\n",
        "            X = X.cuda()\n",
        "            y = y.cuda()\n",
        "            outputs = model(X)\n",
        "\n",
        "            prediction = torch.argmax(outputs, dim=1) # max outputs max and argmax\n",
        "            num_correct = torch.sum(prediction == y)\n",
        "            total_correct += num_correct.item()\n",
        "            total_images += y.shape[0]\n",
        "\n",
        "            total_loss += loss_criterion(outputs, y).item()*y.shape[0]\n",
        "\n",
        "        train_accuracy = total_correct / total_images\n",
        "        train_loss = total_loss / total_images\n",
        "        num_training_images = total_images\n",
        "\n",
        "        total_correct = 0\n",
        "        total_loss = 0\n",
        "        total_images = 0\n",
        "        for X, y in test_loader:\n",
        "            y = y.cuda()\n",
        "            outputs = model(X.cuda())\n",
        "\n",
        "            prediction = torch.argmax(outputs, dim=1) # max outputs max and argmax\n",
        "            num_correct = torch.sum(prediction == y.cuda())\n",
        "            total_correct += num_correct.item()\n",
        "            total_images += y.shape[0]\n",
        "\n",
        "            total_loss += loss_criterion(outputs, y).item()*y.shape[0]\n",
        "\n",
        "        gen_accuracy = total_correct / total_images\n",
        "        gen_loss = total_loss / total_images\n",
        "        num_test_images = total_images\n",
        "        \n",
        "        print(\"Train Accuracy: \", train_accuracy)\n",
        "        print(\"Test Accuracy: \", gen_accuracy)\n",
        "        #  print(\"Test Loss: \", gen_loss)\n",
        "        #  print(\"\\n\\n\")\n",
        "\n",
        "    stats = dict(num_parameters = model.num_parameters(), \n",
        "                 train_accuracy = train_accuracy,\n",
        "                 train_loss = train_loss,\n",
        "                 generalisation_accuracy = gen_accuracy,\n",
        "                 generalisation_loss = gen_loss,\n",
        "                 num_training_images = num_training_images,\n",
        "                 num_test_images = num_test_images)\n",
        "    return stats\n",
        "\n",
        "def flatten(x):\n",
        "    return torch.reshape(x,(-1,))\n",
        "\n",
        "transform = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    #  flatten,\n",
        "    ])\n",
        "\n",
        "train1 = torchvision.datasets.FashionMNIST('~/data/',train=True,transform=transform, download=True)\n",
        "test1 = torchvision.datasets.FashionMNIST('~/data/',train=False,transform=transform, download=True)\n",
        "\n",
        "# hyperparams\n",
        "\n",
        "batch_size = 256\n",
        "num_epochs = 50\n",
        "lr = 0.001\n",
        "momentum = 0.9\n",
        "LOSS_THRESHOLD = 0.3\n",
        "BREAK_TRAINING = False\n",
        "DATASET_SIZE = len(train1)\n",
        "\n",
        "\n",
        "# get data\n",
        "stats_list = []\n",
        "accuracy_list = []\n",
        "# iterate over experiment variations \n",
        "for i in [0]:\n",
        "    exp1_acc = []\n",
        "    # run 10 of each experiment\n",
        "    for _ in range(10):\n",
        "        model = Coordinet(3, 200, 10, [200,200],[100,80])\n",
        "        model = model.cuda()\n",
        "        stats1 = training_loop(model,train1,test1)\n",
        "        exp1_acc.append(stats1['generalisation_accuracy'])\n",
        "        stats_list.append([DATASET_SIZE,stats1])\n",
        "\n",
        "    accuracies_dict = {'n':DATASET_SIZE, 1:exp1_acc, }\n",
        "    print(accuracies_dict)\n",
        "    print([np.mean(accuracies_dict[i]) for i in [1,]])\n",
        "    accuracy_list.append(accuracies_dict)\n",
        "\n",
        "\n",
        "    with open(\"accuracy_list.pickle\", \"wb\") as f:\n",
        "        pickle.dump(accuracy_list,f)\n",
        "\n",
        "    with open(\"stats.pickle\", \"wb\") as f:\n",
        "        pickle.dump(stats_list, f)\n",
        "with open(\"accuracy_list.pickle\", \"wb\") as f:\n",
        "    pickle.dump(accuracy_list,f)\n",
        "\n",
        "with open(\"stats.pickle\", \"wb\") as f:\n",
        "    pickle.dump(stats_list, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Epoch 140, Average Loss: 0.2996609587510427\n",
            "Train Accuracy:  0.8849333333333333\n",
            "Test Accuracy:  0.8541\n",
            " Epoch 26, Average Loss: 0.4994941497008006"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7bc399f8dd74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoordinet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mstats1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0mexp1_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generalisation_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0mstats_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDATASET_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstats1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-7bc399f8dd74>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(model, train_dataset, test_dataset)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m#  Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mcurr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#curr batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-7bc399f8dd74>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoordinate_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# (pixel -> embedding) module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-7bc399f8dd74>\u001b[0m in \u001b[0;36mcoordinate_encoder\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSSyrpWxBEey"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}